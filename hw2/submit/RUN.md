# README

## Introduction
This assignment covers policy gradients and variance reduction techniques like reward-to-go and neural network baselines. Starter code is available at:

[GitHub Repository](https://github.com/LeCAR-Lab/16831-S24-HW.git)

## Implementation Overview
This builds on Homework 1, requiring integration of prior components. New tasks include:

- Implementing return estimators
- Training the policy network
- Running RL experiments
- Implementing neural network baselines
- Testing generalized advantage estimation (GAE)


## Special Instructions
- The folder `automate/` contains various .sh scripts that automate parameter search for different questions.
- Run each script within the `automate/` folder
- To enable parallel execuction of trajectory collection, add the -p flag (this is done in exp9.1.sh)
- Each script is labled according the the question it solves on the HW (starts with exp short for experiment)


